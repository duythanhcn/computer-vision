{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import face_recognition\n",
    "import cv2\n",
    "import os\n",
    "import os.path\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from imutils import paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detectFace(img_path):\n",
    "    image = cv2.imread(img_path)\n",
    "    rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    print(\"[INFO] recognizing faces...\")\n",
    "    boxes = face_recognition.face_locations(rgb,model=\"cnn\")\n",
    "    encodings = face_recognition.face_encodings(rgb, boxes)\n",
    "    names = []\n",
    "    for encoding in encodings:\n",
    "        print(encoding)\n",
    "        matches = face_recognition.compare_faces(data[\"encodings\"],encoding)\n",
    "        name = \"Unknown\"\n",
    "        if True in matches:\n",
    "            matchedIdxs = [i for (i, b) in enumerate(matches) if b]\n",
    "            counts = {}\n",
    "            for i in matchedIdxs:\n",
    "                name = data[\"names\"][i]\n",
    "                counts[name] = counts.get(name, 0) + 1\n",
    "            name = max(counts, key=counts.get)\n",
    "        names.append(name)\n",
    "    \n",
    "    # loop over the recognized faces\n",
    "    for ((top, right, bottom, left), name) in zip(boxes, names):\n",
    "        # draw the predicted face name on the image\n",
    "        cv2.rectangle(image, (left, top), (right, bottom), (0, 255, 0), 2)\n",
    "        y = top - 15 if top - 15 > 15 else top + 15\n",
    "        cv2.putText(image, name, (left, y), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            0.75, (0, 255, 0), 2)\n",
    "\n",
    "    # show the output image\n",
    "    cv2.imshow(\"Image\", image)\n",
    "    cv2.waitKey(0)\n",
    "    \n",
    "# detectFace(\"./input/img1.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recogizeFace(target_face_encoding):\n",
    "#     target_face_encoding = face_recognition.face_encodings(target_face)\n",
    "    # load the known faces and embeddings\n",
    "    print(\"[INFO] loading encodings...\")\n",
    "    data = pickle.loads(open(\"encodings.pickle\", \"rb\").read())\n",
    "    # loop over the facial embeddings\n",
    "    name = \"Unknown\"\n",
    "    matches = face_recognition.compare_faces(data[\"encodings\"], target_face_encoding)\n",
    "    # check to see if we have found a match\n",
    "    if True in matches:\n",
    "        matchedIdxs = [i for (i, b) in enumerate(matches) if b]\n",
    "        counts = {}\n",
    "        for i in matchedIdxs:\n",
    "            name = data[\"names\"][i]\n",
    "            counts[name] = counts.get(name, 0) + 1\n",
    "        name = max(counts, key=counts.get)\n",
    "\n",
    "    return name\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start load dataset to training\n",
      "./dataset/TRUONGTX/IMG_5328_face002.jpg\n",
      "./dataset/TAIHPT/face_19.jpg\n",
      "./dataset/DUYNN/duynn01.jpg\n",
      "./dataset/DUYNN/duynn02.jpg\n",
      "./dataset/TANTD/image_2.jpg\n",
      "./dataset/TANTD/image_3.jpg\n",
      "./dataset/TANTD/image_1.jpg\n",
      "./dataset/TANTD/image_4.jpg\n",
      "./dataset/THAODNT/face_52.jpg\n",
      "./dataset/ThuyLTN/ThuyLTN.png\n",
      "./dataset/MINHHA/face_51.jpg\n",
      "./dataset/MINHHA/face_42.jpg\n",
      "./dataset/MINHHA/face_4.jpg\n",
      "./dataset/MINHHA/face_58.jpg\n",
      "./dataset/DUYETLV/face-5.jpg\n",
      "./dataset/DUYETLV/face-43.jpg\n",
      "./dataset/DUYETLV/Screen Shot 2019-02-27 at 6.39.22 PM.png\n",
      "./dataset/DUYETLV/Screen Shot 2019-02-27 at 6.39.12 PM.png\n",
      "./dataset/DUYETLV/Screen Shot 2019-02-27 at 6.38.53 PM.png\n",
      "./dataset/TANPV/TANPV-2.jpg\n",
      "./dataset/TANPV/TANPV-1.jpg\n",
      "./dataset/LOCTH/locth-01.jpg\n",
      "./dataset/LOCTH/locth-03.jpg\n",
      "./dataset/LOCTH/locth-02.jpg\n",
      "./dataset/VUND/vund2.png\n",
      "./dataset/VUND/vund3.png\n",
      "./dataset/ANHLT/ANHLT-1.jpg\n",
      "./dataset/ANHLT/ANHLT-2.jpg\n",
      "./dataset/NHANTH/face_50.jpg\n",
      "./dataset/MYNH/face_77.jpg\n",
      "./dataset/VULQ/vulq04.jpg\n",
      "./dataset/VULQ/vulq01.jpg\n",
      "./dataset/VULQ/vulq03.jpg\n",
      "./dataset/VULQ/vulq02.jpg\n",
      "./dataset/QUANVM/2.png\n",
      "./dataset/QUANVM/1.png\n",
      "./dataset/QUANVM/0.png\n",
      "./dataset/HUNGNV/face15.jpg\n",
      "./dataset/HUNGNV/face65.jpg\n",
      "./dataset/TIENBDT/8932.jpg\n",
      "./dataset/HOBV/4.jpg\n",
      "./dataset/HOBV/5.jpg\n",
      "./dataset/HOBV/6.jpg\n",
      "./dataset/HOBV/2.jpg\n",
      "./dataset/HOBV/3.jpg\n",
      "./dataset/HOBV/1.jpg\n",
      "./dataset/HOAISD/face_21.jpg\n",
      "./dataset/CUONGNT/CUONGNT-3.jpg\n",
      "./dataset/CUONGNT/CUONGNT-2.jpg\n",
      "./dataset/CUONGNT/CUONGNT-1.jpg\n",
      "./dataset/TULG/face_46.jpg\n",
      "./dataset/TULG/face_102.jpg\n",
      "./dataset/TULG/face_34.jpg\n",
      "./dataset/TULG/face_109.jpg\n",
      "./dataset/TULG/face_66.jpg\n",
      "./dataset/HANTQ/HANTQ.jpg\n",
      "encoding done\n",
      "start write data to file\n",
      "[INFO] serializing encodings...\n",
      "training done\n"
     ]
    }
   ],
   "source": [
    "import face_recognition\n",
    "import cv2\n",
    "import os\n",
    "import os.path\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from imutils import paths\n",
    "\n",
    "def trainModel1():\n",
    "    path = './dataset'\n",
    "    knownEncodings = []\n",
    "    knownNames = []\n",
    "    # r=root, d=directories, f = files\n",
    "    index = 0\n",
    "    print(\"start load dataset to training\")\n",
    "    for r, d, f in os.walk(path):\n",
    "        for folder in d:\n",
    "            name = folder\n",
    "            for r1, d1, f1 in os.walk(os.path.join(r, folder)):\n",
    "                for file in f1:\n",
    "                    print(os.path.join(r, folder, file))\n",
    "                    image_loaded = face_recognition.load_image_file(os.path.join(r, folder, file))\n",
    "                    face_encodings = face_recognition.face_encodings(image_loaded)\n",
    "                    # loop over the encodings\n",
    "                    for encoding in face_encodings:\n",
    "                        # add each encoding + name to our set of known names and\n",
    "                        # encodings\n",
    "                        knownEncodings.append(encoding)\n",
    "                        knownNames.append(name)\n",
    "    print(\"encoding done\")\n",
    "    print(\"start write data to file\")\n",
    "    print(\"[INFO] serializing encodings...\")\n",
    "    data = {\"encodings\": knownEncodings, \"names\": knownNames}\n",
    "    f = open(\"encodings.pickle\", \"wb\")\n",
    "    f.write(pickle.dumps(data))\n",
    "    f.close()\n",
    "    print(\"training done\")\n",
    "    \n",
    "trainModel1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] quantifying faces...\n",
      "[INFO] processing image 1/56\n",
      "TRUONGTX\n",
      "[INFO] processing image 2/56\n",
      "TAIHPT\n",
      "[INFO] processing image 3/56\n",
      "DUYNN\n",
      "[INFO] processing image 4/56\n",
      "DUYNN\n",
      "[INFO] processing image 5/56\n",
      "TANTD\n",
      "[INFO] processing image 6/56\n",
      "TANTD\n",
      "[INFO] processing image 7/56\n",
      "TANTD\n",
      "[INFO] processing image 8/56\n",
      "TANTD\n",
      "[INFO] processing image 9/56\n",
      "THAODNT\n",
      "[INFO] processing image 10/56\n",
      "ThuyLTN\n",
      "[INFO] processing image 11/56\n",
      "MINHHA\n",
      "[INFO] processing image 12/56\n",
      "MINHHA\n",
      "[INFO] processing image 13/56\n",
      "MINHHA\n",
      "[INFO] processing image 14/56\n",
      "MINHHA\n",
      "[INFO] processing image 15/56\n",
      "DUYETLV\n",
      "[INFO] processing image 16/56\n",
      "DUYETLV\n",
      "[INFO] processing image 17/56\n",
      "DUYETLV\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(3.4.3) /Users/travis/build/skvark/opencv-python/opencv/modules/imgproc/src/color.cpp:181: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-62c8c98f3ef7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0mtrainModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-62c8c98f3ef7>\u001b[0m in \u001b[0;36mtrainModel\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m# to dlib ordering (RGB)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimagePath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mrgb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;31m# detect the (x, y)-coordinates of the bounding boxes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m# corresponding to each face in the input image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(3.4.3) /Users/travis/build/skvark/opencv-python/opencv/modules/imgproc/src/color.cpp:181: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"
     ]
    }
   ],
   "source": [
    "import face_recognition\n",
    "import cv2\n",
    "import os\n",
    "import os.path\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from imutils import paths\n",
    "\n",
    "def trainModel():\n",
    "    # grab the paths to the input images in our dataset\n",
    "    print(\"[INFO] quantifying faces...\")\n",
    "    imagePaths = list(paths.list_images(\"./dataset\"))\n",
    "\n",
    "    # initialize the list of known encodings and known names\n",
    "    knownEncodings = []\n",
    "    knownNames = []\n",
    "    # loop over the image paths\n",
    "    for (i, imagePath) in enumerate(imagePaths):\n",
    "        # extract the person name from the image path\n",
    "        print(\"[INFO] processing image {}/{}\".format(i + 1, len(imagePaths)))\n",
    "        name = imagePath.split(os.path.sep)[-2]\n",
    "        print(name)\n",
    "        # load the input image and convert it from BGR (OpenCV ordering)\n",
    "        # to dlib ordering (RGB)\n",
    "        image = cv2.imread(imagePath)\n",
    "        rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        # detect the (x, y)-coordinates of the bounding boxes\n",
    "        # corresponding to each face in the input image\n",
    "        boxes = face_recognition.face_locations(rgb, model=\"cnn\")\n",
    "\n",
    "        # compute the facial embedding for the face\n",
    "        encodings = face_recognition.face_encodings(rgb, boxes)\n",
    "\n",
    "        # loop over the encodings\n",
    "        for encoding in encodings:\n",
    "            # add each encoding + name to our set of known names and\n",
    "            # encodings\n",
    "            knownEncodings.append(encoding)\n",
    "            knownNames.append(name)\n",
    "            \n",
    "    # dump the facial encodings + names to disk\n",
    "    print(\"[INFO] serializing encodings...\")\n",
    "    data = {\"encodings\": knownEncodings, \"names\": knownNames}\n",
    "    f = open(\"encodings.pickle\", \"wb\")\n",
    "    f.write(pickle.dumps(data))\n",
    "    f.close()\n",
    "    \n",
    "#trainModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
